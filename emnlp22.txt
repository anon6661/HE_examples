虽然符合人类的阅读习惯，把这种先验作为额外的限制 不利于忠实性
因为时间关系设计了新的简单的方法


In NLP, most of existing work on local
explanation generation focuses on producing word-
level or phrase-level explanations by quantifying
contributions of individual words or phrases to a
model predictio
However, neither of them is able to explain the
model decision-making in terms of how words and
phrases are interacted with each other and com-
posed together for the final prediction.

An explanation
being able to answer this question will give users a
better understanding on the model decision-making
Given a pre-
diction from a trained DNN, ACD produces a hierarchical clustering of the input features, along
with the contribution of each cluster to the final prediction

Second, most importantly,
we introduce the idea of hierarchical saliency, where a group-level importance measure

In doing so, ACD aims to be informative enough to capture meaningful feature interactions
while displaying a sufficiently small subset of all feature groups to maintain simplicity

these algorithms generate hierarchical explanation on how the model captures
compositional semantics (e.g., stress or negation) in making predictions
